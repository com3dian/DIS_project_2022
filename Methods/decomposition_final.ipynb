{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SPARK_HOME\"] = \"/home/com3dian/Documents/github/Period4/DIS/spark-3.2.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## You can add more config while building \n",
    "spark = SparkSession.builder.master(\"local[16]\").\\\n",
    "                    config(\"spark.app.name\",\"session_one\").\\\n",
    "                    getOrCreate() #number of threads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import DataFrame\n",
    "from tqdm import tqdm\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2NumpyArray(df, colName):\n",
    "    '''\n",
    "    convert spark dataframe to numpy array\n",
    "    '''\n",
    "    return np.array(df.select(colName).collect())\n",
    "\n",
    "\n",
    "def numpyArray2Matrix(array):\n",
    "    '''\n",
    "    convert numpy array to spark Rowmatrix\n",
    "    ----------------------------\n",
    "    return: Rowmatrix\n",
    "    '''\n",
    "    if len(array.shape) == 3:\n",
    "        array.reshape((array.shape[0], array.shape[-1]))\n",
    "    \n",
    "    \n",
    "    denseVectorList = []\n",
    "    for i in range(len(array)):\n",
    "        denseVectorList.append(Vectors.dense(array[i]))\n",
    "        \n",
    "    \n",
    "    RDD = spark.sparkContext.parallelize(denseVectorList)\n",
    "    normVectors = RDD.map(lambda x: x/(np.linalg.norm(x, 2)))\n",
    "    \n",
    "    RDD = spark.sparkContext.parallelize(normVectors.collect())\n",
    "    matrix = RowMatrix(RDD)\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def SVDsimilarity(matrix, numDimension = 1, normalization = False):\n",
    "    '''\n",
    "    generalized cosine similarity using SVD(singular value decomposition)\n",
    "    by doing SVD, the input matrix Y will be decomposited into 3 matrix: U, S, V, with Y = USV^T\n",
    "    where S can be considered as a lower rank approximation of Y\n",
    "    the SVD optimal in the sense that minimizing the Frobinius norm of reconstruction error || \\hat{Y} - Y ||^{2}_{F}\n",
    "    therefore, by comparing the 'order K coefficient of determination' \\frac{||\\hat{Y} ||^2_F}{||Y ||^2_F}, we shall a similarity.\n",
    "    ----------------------\n",
    "    in the case of only 2 vectors, the SVD similarity is equal to the cosine similarity\n",
    "    ----------------------\n",
    "    the original SVD similarity is ranged from 1/n to 1, where n is the number of vectors\n",
    "    to get it can range over the entire [0,1] interval, one can normalize it by \\frac{}{} if only using the first sigular value\n",
    "    \n",
    "    ------------------------------------------\n",
    "    matrix: pyspark RowMatrix, represents a row-oriented distributed Matrix with no meaningful row indices\n",
    "            each column/row is an input vector\n",
    "            all element in matrix should be positive\n",
    "    numDimension: integer, if not 1 then use the first(largest) few singular value\n",
    "    normalization: if true then do normalization\n",
    "    \n",
    "    '''\n",
    "    N = matrix.numRows()\n",
    "    # SVD\n",
    "    svd = matrix.computeSVD(numDimension, computeU=False)\n",
    "    sVector = svd.s.toArray()\n",
    "    YApproximate = np.sum(sVector*sVector)**0.5\n",
    "    \n",
    "    GramianMatrix = matrix.computeGramianMatrix().toArray()\n",
    "    Y = np.trace(GramianMatrix)**0.5\n",
    "    \n",
    "    # normalization\n",
    "    if not normalization:\n",
    "        similarityScore = YApproximate/Y\n",
    "    else:\n",
    "        similarityScore = ((YApproximate/Y * N) - 1)/(N -1)\n",
    "        \n",
    "    # return 2 * similarityScore**2 - 1\n",
    "    # double angle formula\n",
    "    return 2 * similarityScore**2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossHomogeneityScore(df, queryColName, featureColName):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if queryColName not in df.schema.names:\n",
    "        \n",
    "        npArray = dataframe2NumpyArray(df, featureColName)\n",
    "        matrix = numpyArray2Matrix(npArray)\n",
    "        similarity = SVDsimilarity(matrix)\n",
    "        return similarity\n",
    "    \n",
    "    totalRows = df.count()\n",
    "    queries = list(set(df.select(queryColName).collect()))\n",
    "    homogeneityScore = 0\n",
    "    \n",
    "    for query in tqdm(queries):\n",
    "        # get each cluster\n",
    "        dfQuery = df.filter(df[queryColName] == query.query)\n",
    "        # get number of rows\n",
    "        numRows = dfQuery.count()\n",
    "        npArray = dataframe2NumpyArray(dfQuery, featureColName)\n",
    "        matrix = numpyArray2Matrix(npArray)\n",
    "        similarity = SVDsimilarity(matrix)\n",
    "        homogeneityScore += similarity * numRows/totalRows\n",
    "    \n",
    "    return homogeneityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfFrequencyTables(df): #take main dataframe, generate frequency dataframes\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    histograms = []\n",
    "    for col in df.dtypes:\n",
    "        h=df.groupBy(col[0]).count()\n",
    "        h = h.sort(desc(\"count\"))\n",
    "        histograms.append(h)\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDecompFromTopFrequencies(df, histograms):\n",
    "    clusterlst=[]\n",
    "    for i in range(len(histograms)): #query database with top values of all columns\n",
    "        d= str(histograms[i].first()) #value of the first row\n",
    "        #print(d)\n",
    "        d = d.split(\",\")[0].split('=')[1] #the splits are for formatting the string\n",
    "        #print(d)\n",
    "        #print(\"d before:\",d)\n",
    "        if \"'\"  in d:\n",
    "            d = d.split(\"'\")[1]\n",
    "        \n",
    "        #print(\"d after:\",d)\n",
    "        #print(type(d))\n",
    "        cname = str(histograms[i][0]).split(\"'\")[1]\n",
    "        data = (df.filter(col(cname) == d))\n",
    "        clusterlst.append(data)\n",
    "    return clusterlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_dfs_equal(df1, df2): #this works, i tested it\n",
    "    res = df1.subtract(df2) #set subtraction on the two dataframes. \n",
    "    if res.count() == 0: #subtraction yielded empty set\n",
    "        print(\"dataframes are equal\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"error! these rows are not in the union of your queries:\")\n",
    "        res.show() #show which tuples are not included in your query union\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def getDecompUsingFreqTable(df, freqdf):\n",
    "    colName = freqdf.columns[0]\n",
    "    \n",
    "    df = df.withColumn('new_query_1', lit(colName))\n",
    "    df = df.withColumn('new_query', concat_ws(' = ', 'new_query_1', colName))\n",
    "    df = df.drop('new_query_1')\n",
    "    \n",
    "    if 'query' in df.columns:\n",
    "        df = df.withColumn('new_query', concat_ws(', ', 'new_query', 'query'))\n",
    "        df = df.drop('query')\n",
    "    df = df.withColumnRenamed('new_query', 'query')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatureVector(df): #get feature vector for any dataframe for homogeneity function\n",
    "    string_cols = [c for c, t in df.dtypes if t =='string' and c != 'query'] #get all columns that have stringtype, except query column\n",
    "    \n",
    "    stringindex_cols = [(i + \"_indexed\") for i in string_cols]\n",
    "    indexer  = StringIndexer( inputCols=string_cols, outputCols=stringindex_cols, handleInvalid='error', stringOrderType='frequencyDesc')\n",
    "    indexer.setHandleInvalid(\"keep\") #change to \"skip\" to remove problematic rows\n",
    "    indexed = indexer.fit(df).transform(df) #dataframe with indexed columns attached\n",
    "    \n",
    "    allnonstringcols = [column.name for column in indexed.schema if column.dataType != StringType()]\n",
    "    vecAssembler = VectorAssembler(inputCols = allnonstringcols, outputCol=\"features_unscaled\") #all numerical columns are put into feature vector, including indexed cols\n",
    "    \n",
    "    result = ( vecAssembler.transform(indexed)) #return the dataframe with feature column attached\n",
    "    scaler = MinMaxScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n",
    "    result = scaler.fit(result).transform(result)\n",
    "    result = result.drop(\"features_unscaled\")\n",
    "    for col in stringindex_cols:\n",
    "        result = result.drop(col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(hist):\n",
    "    return sorted(hist, key = lambda x: x.count())\n",
    "\n",
    "def getDecompositionbyColumn(df, K):\n",
    "    histograms = shuffle(listOfFrequencyTables(df))\n",
    "    histList = list(range(len(histograms)))\n",
    "    colLeft = len(histograms)\n",
    "    \n",
    "    decomUnionWithVec = addFeatureVector(df)\n",
    "    overAllHomoScore = crossHomogeneityScore(decomUnionWithVec, 'query', 'features')\n",
    "    print('over all homogeneity score: ', overAllHomoScore)\n",
    "    \n",
    "    nBucketsBefore = 1\n",
    "    crossScoreBefore = overAllHomoScore\n",
    "    \n",
    "    update = True\n",
    "    \n",
    "    while update:\n",
    "        if nBucketsBefore >= K or colLeft == 0:\n",
    "\n",
    "            break\n",
    "        \n",
    "        \n",
    "        update = False\n",
    "        removeIndex = -1\n",
    "        \n",
    "        for i in histList:\n",
    "            freqdf = histograms[i]\n",
    "            \n",
    "            unionWithVec = getDecompUsingFreqTable(decomUnionWithVec, freqdf)\n",
    "            nBucketsAfter = unionWithVec.select('query').distinct().count()\n",
    "            \n",
    "            if nBucketsAfter == nBucketsBefore or nBucketsAfter > K:\n",
    "                continue\n",
    "            \n",
    "            crossScoreAfter = crossHomogeneityScore(unionWithVec, 'query', 'features')\n",
    "            if crossScoreAfter > crossScoreBefore :\n",
    "                \n",
    "                # update using new decomposition\n",
    "                \n",
    "                crossScoreBefore = crossScoreAfter\n",
    "                \n",
    "                removeIndex = i \n",
    "                update = True\n",
    "                \n",
    "        histList = [_ for _ in histList if _ != removeIndex]\n",
    "        colLeft = len(histList)\n",
    "        \n",
    "        if update:\n",
    "            \n",
    "            decomUnionWithVec = getDecompUsingFreqTable(decomUnionWithVec, histograms[removeIndex])\n",
    "            nBucketsBefore = decomUnionWithVec.select('query').distinct().count()\n",
    "        \n",
    "    if 'query' not in decomUnionWithVec.columns:\n",
    "        print('user requested K =', str(K), ', but we can only got ', str(1), 'clusters.') \n",
    "        return decomUnionWithVec.drop('features')\n",
    "    \n",
    "    if nBucketsBefore != K:\n",
    "        print('user requested K =', str(K), ', but we can only got ', str(nBucketsBefore), 'clusters.') \n",
    "    crossScoreAfter = crossHomogeneityScore(decomUnionWithVec, 'query', 'features')\n",
    "    print('after decomposition homogeneity score: ', crossScoreAfter)\n",
    "    return decomUnionWithVec.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|LatD| \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"|            \"City\"| \"State\"|\n",
      "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|41.0|    5.0|   59.0|  \"N\"|   80.0|   39.0|    0.0|  \"W\"|      \"Youngstown\"|      OH|\n",
      "|42.0|   52.0|   48.0|  \"N\"|   97.0|   23.0|   23.0|  \"W\"|         \"Yankton\"|      SD|\n",
      "|46.0|   35.0|   59.0|  \"N\"|  120.0|   30.0|   36.0|  \"W\"|          \"Yakima\"|      WA|\n",
      "|42.0|   16.0|   12.0|  \"N\"|   71.0|   48.0|    0.0|  \"W\"|       \"Worcester\"|      MA|\n",
      "|43.0|   37.0|   48.0|  \"N\"|   89.0|   46.0|   11.0|  \"W\"| \"Wisconsin Dells\"|      WI|\n",
      "|36.0|    5.0|   59.0|  \"N\"|   80.0|   15.0|    0.0|  \"W\"|   \"Winston-Salem\"|      NC|\n",
      "|49.0|   52.0|   48.0|  \"N\"|   97.0|    9.0|    0.0|  \"W\"|        \"Winnipeg\"|      MB|\n",
      "|39.0|   11.0|   23.0|  \"N\"|   78.0|    9.0|   36.0|  \"W\"|      \"Winchester\"|      VA|\n",
      "|34.0|   14.0|   24.0|  \"N\"|   77.0|   55.0|   11.0|  \"W\"|      \"Wilmington\"|      NC|\n",
      "|39.0|   45.0|    0.0|  \"N\"|   75.0|   33.0|    0.0|  \"W\"|      \"Wilmington\"|      DE|\n",
      "|48.0|    9.0|    0.0|  \"N\"|  103.0|   37.0|   12.0|  \"W\"|       \"Williston\"|      ND|\n",
      "|41.0|   15.0|    0.0|  \"N\"|   77.0|    0.0|    0.0|  \"W\"|    \"Williamsport\"|      PA|\n",
      "|37.0|   40.0|   48.0|  \"N\"|   82.0|   16.0|   47.0|  \"W\"|      \"Williamson\"|      WV|\n",
      "|33.0|   54.0|    0.0|  \"N\"|   98.0|   29.0|   23.0|  \"W\"|   \"Wichita Falls\"|      TX|\n",
      "|37.0|   41.0|   23.0|  \"N\"|   97.0|   20.0|   23.0|  \"W\"|         \"Wichita\"|      KS|\n",
      "|40.0|    4.0|   11.0|  \"N\"|   80.0|   43.0|   12.0|  \"W\"|        \"Wheeling\"|      WV|\n",
      "|26.0|   43.0|   11.0|  \"N\"|   80.0|    3.0|    0.0|  \"W\"| \"West Palm Beach\"|      FL|\n",
      "|47.0|   25.0|   11.0|  \"N\"|  120.0|   19.0|   11.0|  \"W\"|       \"Wenatchee\"|      WA|\n",
      "|41.0|   25.0|   11.0|  \"N\"|  122.0|   23.0|   23.0|  \"W\"|            \"Weed\"|      CA|\n",
      "|31.0|   13.0|   11.0|  \"N\"|   82.0|   20.0|   59.0|  \"W\"|        \"Waycross\"|      GA|\n",
      "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/com3dian/Documents/github/DIS_project_2022/data/cities.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over all homogeneity score:  0.5641430012145967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.13it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user requested K = 12 , but we can only got  10 clusters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after decomposition homogeneity score:  0.6603351770763216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = getDecompositionbyColumn(df, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|query         |\n",
      "+--------------+\n",
      "| \"LonS\" = 23.0|\n",
      "| \"LonS\" = 48.0|\n",
      "| \"LonS\" = 59.0|\n",
      "| \"LonS\" = 11.0|\n",
      "| \"LonS\" = 35.0|\n",
      "| \"LonS\" = 12.0|\n",
      "| \"LonS\" = 0.0 |\n",
      "| \"LonS\" = 24.0|\n",
      "| \"LonS\" = 47.0|\n",
      "| \"LonS\" = 36.0|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('query').distinct().show(result.select('query').distinct().count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(' \"City\"').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "|LatD| \"LatM\"| \"LatS\"|\n",
      "+----+-------+-------+\n",
      "|41.0|    5.0|   59.0|\n",
      "|42.0|   52.0|   48.0|\n",
      "|46.0|   35.0|   59.0|\n",
      "|42.0|   16.0|   12.0|\n",
      "|43.0|   37.0|   48.0|\n",
      "|36.0|    5.0|   59.0|\n",
      "|49.0|   52.0|   48.0|\n",
      "|39.0|   11.0|   23.0|\n",
      "|34.0|   14.0|   24.0|\n",
      "|39.0|   45.0|    0.0|\n",
      "|48.0|    9.0|    0.0|\n",
      "|41.0|   15.0|    0.0|\n",
      "|37.0|   40.0|   48.0|\n",
      "|33.0|   54.0|    0.0|\n",
      "|37.0|   41.0|   23.0|\n",
      "|40.0|    4.0|   11.0|\n",
      "|26.0|   43.0|   11.0|\n",
      "|47.0|   25.0|   11.0|\n",
      "|41.0|   25.0|   11.0|\n",
      "|31.0|   13.0|   11.0|\n",
      "+----+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_only_test = df\n",
    "for col in list(df.columns[3:]):\n",
    "    df_only_test = df_only_test.drop(col)\n",
    "df_only_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----+-------+-------+-------+-----+-------+--------+\n",
      "|LatD| \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"| \"City\"| \"State\"|\n",
      "+----+-------+-------+-----+-------+-------+-------+-----+-------+--------+\n",
      "|  25|     51|     10|    1|     44|     53|     10|    1|    120|      47|\n",
      "+----+-------+-------+-----+-------+-------+-------+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "(df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over all homogeneity score:  0.6423404420799437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.20it/s]\n",
      "100%|██████████| 51/51 [00:41<00:00,  1.23it/s]\n",
      " 48%|████▊     | 36/75 [00:28<00:32,  1.20it/s]"
     ]
    }
   ],
   "source": [
    "result = getDecompositionbyColumn(df_only_test, 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select('query').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
