{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!java -version\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "YZDw7dwzbEZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fd0mONRAadFi"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "## You can add more config while building \n",
        "spark = SparkSession.builder.master(\"local[8]\").\\\n",
        "                    config(\"spark.app.name\",\"session_one\").\\\n",
        "                    getOrCreate() #number of threads = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "waVNn5omadFj"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F6HNRFs8adFj"
      },
      "outputs": [],
      "source": [
        "def dataframe2NumpyArray(df, colName):\n",
        "    '''\n",
        "    convert spark dataframe to numpy array\n",
        "    '''\n",
        "    return np.array(df.select(colName).collect())\n",
        "\n",
        "\n",
        "def numpyArray2Matrix(array):\n",
        "    '''\n",
        "    convert numpy array to spark Rowmatrix\n",
        "    ----------------------------\n",
        "    return: Rowmatrix\n",
        "    '''\n",
        "    if len(array.shape) == 3:\n",
        "        array.reshape((array.shape[0], array.shape[-1]))\n",
        "    \n",
        "    \n",
        "    denseVectorList = []\n",
        "    for i in range(len(array)):\n",
        "        denseVectorList.append(Vectors.dense(array[i]))\n",
        "        \n",
        "    \n",
        "    RDD = spark.sparkContext.parallelize(denseVectorList)\n",
        "    normVectors = RDD.map(lambda x: x/(np.linalg.norm(x, 2)))\n",
        "    \n",
        "    RDD = spark.sparkContext.parallelize(normVectors.collect())\n",
        "    matrix = RowMatrix(RDD)\n",
        "        \n",
        "    return matrix\n",
        "\n",
        "\n",
        "def SVDsimilarity(matrix, numDimension = 1, normalization = False):\n",
        "    '''\n",
        "    generalized cosine similarity using SVD(singular value decomposition)\n",
        "    by doing SVD, the input matrix Y will be decomposited into 3 matrix: U, S, V, with Y = USV^T\n",
        "    where S can be considered as a lower rank approximation of Y\n",
        "    the SVD optimal in the sense that minimizing the Frobinius norm of reconstruction error || \\hat{Y} - Y ||^{2}_{F}\n",
        "    therefore, by comparing the 'order K coefficient of determination' \\frac{||\\hat{Y} ||^2_F}{||Y ||^2_F}, we shall a similarity.\n",
        "    ----------------------\n",
        "    in the case of only 2 vectors, the SVD similarity is equal to the cosine similarity\n",
        "    ----------------------\n",
        "    the original SVD similarity is ranged from 1/n to 1, where n is the number of vectors\n",
        "    to get it can range over the entire [0,1] interval, one can normalize it by \\frac{}{} if only using the first sigular value\n",
        "    \n",
        "    ------------------------------------------\n",
        "    matrix: pyspark RowMatrix, represents a row-oriented distributed Matrix with no meaningful row indices\n",
        "            each column/row is an input vector\n",
        "            all element in matrix should be positive\n",
        "    numDimension: integer, if not 1 then use the first(largest) few singular value\n",
        "    normalization: if true then do normalization\n",
        "    \n",
        "    '''\n",
        "    N = matrix.numRows()\n",
        "    # SVD\n",
        "    svd = matrix.computeSVD(numDimension, computeU=False)\n",
        "    sVector = svd.s.toArray()\n",
        "    YApproximate = np.sum(sVector*sVector)**0.5\n",
        "    \n",
        "    GramianMatrix = matrix.computeGramianMatrix().toArray()\n",
        "    Y = np.trace(GramianMatrix)**0.5\n",
        "    \n",
        "    # normalization\n",
        "    if not normalization:\n",
        "        similarityScore = YApproximate/Y\n",
        "    else:\n",
        "        similarityScore = ((YApproximate/Y * N) - 1)/(N -1)\n",
        "        \n",
        "    # return 2 * similarityScore**2 - 1\n",
        "    # double angle formula\n",
        "    return 2 * similarityScore**2 - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cwokl1UqadFk"
      },
      "outputs": [],
      "source": [
        "def crossHomogeneityScore(df, queryColName, featureColName):\n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "    if queryColName not in df.schema.names:\n",
        "        \n",
        "        npArray = dataframe2NumpyArray(df, featureColName)\n",
        "        matrix = numpyArray2Matrix(npArray)\n",
        "        similarity = SVDsimilarity(matrix)\n",
        "        \n",
        "        return similarity\n",
        "    \n",
        "    \n",
        "    totalRows = df.count()\n",
        "    queries = list(set(df.select(queryColName).collect()))\n",
        "    \n",
        "    homogeneityScore = 0\n",
        "    \n",
        "    for query in queries:\n",
        "        # get each cluster\n",
        "        dfQuery = df.filter(df[queryColName] == query.query)\n",
        "        # get number of rows\n",
        "        numRows = dfQuery.count()\n",
        "        \n",
        "        npArray = dataframe2NumpyArray(dfQuery, featureColName)\n",
        "        matrix = numpyArray2Matrix(npArray)\n",
        "        \n",
        "        similarity = SVDsimilarity(matrix)\n",
        "        homogeneityScore += similarity * numRows/totalRows\n",
        "    \n",
        "    return homogeneityScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBJrJkK5adFk",
        "outputId": "88d4c312-bea9-4a75-a6a8-dc84f848fd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
            "|LatD| \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"|            \"City\"| \"State\"|\n",
            "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
            "|41.0|    5.0|   59.0|  \"N\"|   80.0|   39.0|    0.0|  \"W\"|      \"Youngstown\"|      OH|\n",
            "|42.0|   52.0|   48.0|  \"N\"|   97.0|   23.0|   23.0|  \"W\"|         \"Yankton\"|      SD|\n",
            "|46.0|   35.0|   59.0|  \"N\"|  120.0|   30.0|   36.0|  \"W\"|          \"Yakima\"|      WA|\n",
            "|42.0|   16.0|   12.0|  \"N\"|   71.0|   48.0|    0.0|  \"W\"|       \"Worcester\"|      MA|\n",
            "|43.0|   37.0|   48.0|  \"N\"|   89.0|   46.0|   11.0|  \"W\"| \"Wisconsin Dells\"|      WI|\n",
            "|36.0|    5.0|   59.0|  \"N\"|   80.0|   15.0|    0.0|  \"W\"|   \"Winston-Salem\"|      NC|\n",
            "|49.0|   52.0|   48.0|  \"N\"|   97.0|    9.0|    0.0|  \"W\"|        \"Winnipeg\"|      MB|\n",
            "|39.0|   11.0|   23.0|  \"N\"|   78.0|    9.0|   36.0|  \"W\"|      \"Winchester\"|      VA|\n",
            "|34.0|   14.0|   24.0|  \"N\"|   77.0|   55.0|   11.0|  \"W\"|      \"Wilmington\"|      NC|\n",
            "|39.0|   45.0|    0.0|  \"N\"|   75.0|   33.0|    0.0|  \"W\"|      \"Wilmington\"|      DE|\n",
            "|48.0|    9.0|    0.0|  \"N\"|  103.0|   37.0|   12.0|  \"W\"|       \"Williston\"|      ND|\n",
            "|41.0|   15.0|    0.0|  \"N\"|   77.0|    0.0|    0.0|  \"W\"|    \"Williamsport\"|      PA|\n",
            "|37.0|   40.0|   48.0|  \"N\"|   82.0|   16.0|   47.0|  \"W\"|      \"Williamson\"|      WV|\n",
            "|33.0|   54.0|    0.0|  \"N\"|   98.0|   29.0|   23.0|  \"W\"|   \"Wichita Falls\"|      TX|\n",
            "|37.0|   41.0|   23.0|  \"N\"|   97.0|   20.0|   23.0|  \"W\"|         \"Wichita\"|      KS|\n",
            "|40.0|    4.0|   11.0|  \"N\"|   80.0|   43.0|   12.0|  \"W\"|        \"Wheeling\"|      WV|\n",
            "|26.0|   43.0|   11.0|  \"N\"|   80.0|    3.0|    0.0|  \"W\"| \"West Palm Beach\"|      FL|\n",
            "|47.0|   25.0|   11.0|  \"N\"|  120.0|   19.0|   11.0|  \"W\"|       \"Wenatchee\"|      WA|\n",
            "|41.0|   25.0|   11.0|  \"N\"|  122.0|   23.0|   23.0|  \"W\"|            \"Weed\"|      CA|\n",
            "|31.0|   13.0|   11.0|  \"N\"|   82.0|   20.0|   59.0|  \"W\"|        \"Waycross\"|      GA|\n",
            "+----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"cities.csv\",header=True,inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PKg9rrgjadFl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "def listOfFrequencyTables(df): #take main dataframe, generate frequency dataframes\n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "    histograms = []\n",
        "    for col in df.dtypes:\n",
        "        h=df.groupBy(col[0]).count()\n",
        "        h = h.sort(desc(\"count\"))\n",
        "        histograms.append(h)\n",
        "        #h.show() #comment this line to suppress output\n",
        "    return histograms\n",
        "\n",
        "histograms = listOfFrequencyTables(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QqNrqMoaadFl"
      },
      "outputs": [],
      "source": [
        "def getDecompFromTopFrequencies(df, histograms):\n",
        "    clusterlst=[]\n",
        "    for i in range(len(histograms)): #query database with top values of all columns\n",
        "        d= str(histograms[i].first()) #value of the first row\n",
        "        #print(d)\n",
        "        d = d.split(\",\")[0].split('=')[1] #the splits are for formatting the string\n",
        "        #print(d)\n",
        "        #print(\"d before:\",d)\n",
        "        if \"'\"  in d:\n",
        "            d = d.split(\"'\")[1]\n",
        "        \n",
        "        #print(\"d after:\",d)\n",
        "        #print(type(d))\n",
        "        cname = str(histograms[i][0]).split(\"'\")[1]\n",
        "        #print(cname,\"=\",d)\n",
        "        \n",
        "        data = (df.filter(col(cname) == d))\n",
        "        \n",
        "        #data.show(15)\n",
        "        clusterlst.append(data)\n",
        "    return clusterlst\n",
        "\n",
        "clusterlst = getDecompFromTopFrequencies(df, histograms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "scrolled": false,
        "id": "B8su0OwdadFl"
      },
      "outputs": [],
      "source": [
        "def are_dfs_equal(df1, df2): #this works, i tested it\n",
        "    res = df1.subtract(df2) #set subtraction on the two dataframes. \n",
        "    if res.count() == 0: #subtraction yielded empty set\n",
        "        print(\"dataframes are equal\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"error! these rows are not in the union of your queries:\")\n",
        "        res.show() #show which tuples are not included in your query union\n",
        "        return False\n",
        "\n",
        "from functools import reduce\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def getDecompUsingFreqTable(df,freqdf): #takes original database and one frequency table as input, returns union of all queried dataframes as output\n",
        "    #print(\"this is frequency table:\")\n",
        "    #freqdf.show()\n",
        "    cname = freqdf.columns[0]\n",
        "    valuelist = (freqdf.select(freqdf.columns[0]).rdd.flatMap(lambda x: x).collect()) #list of all values of frequency column\n",
        "    \n",
        "    unionlst = []\n",
        "    \n",
        "    for v in valuelist: #each unique value in the freq. table is used as a query\n",
        "        result = df.filter(col(cname) == v)\n",
        "        querystr = cname + \"=\" + str(v)\n",
        "        #print(\"querystr:\",querystr)\n",
        "        \n",
        "        containsquery = False\n",
        "        for c in df.columns: #check if query column exists in the input dataframe\n",
        "            if \"query\" in c:\n",
        "                containsquery= True\n",
        "        \n",
        "        if(containsquery): #check if query column already exists in the input\n",
        "            result=result.withColumn(\"query1\",lit(querystr))\n",
        "            #print(\"first newres:\")\n",
        "            \n",
        "            result= result.withColumn(\"joined\",concat(concat(col(\"query\"), lit(\",\"), col(\"query1\")))) #putting query with existing queries\n",
        "            #print(\"after join\")\n",
        "            \n",
        "            columns_to_drop = ['query', 'query1']\n",
        "            \n",
        "            result=result.drop('query')\n",
        "            result=result.drop('query1')\n",
        "            result=result.withColumnRenamed(\"joined\",\"query\")\n",
        "            \n",
        "            result=result\n",
        "        else:\n",
        "            #print(\"creating query column:\")\n",
        "            result=result.withColumn(\"query\",lit(querystr))\n",
        "        print(querystr)\n",
        "        \n",
        "        unionlst.append(result)\n",
        "    unn = reduce(DataFrame.unionAll, unionlst) #put all queried dataframes back together as one\n",
        "    \n",
        "    \n",
        "    return unn\n",
        "\n",
        "\n",
        "#union = (getDecompUsingFreqTable(df,histograms[0])) #function call with 'Name' frequency table\n",
        "#print(\"equality result:\")\n",
        "#subdf = are_dfs_equal(df,union.drop('query')) #checks if union of queries covers whole database\n",
        "#print(subdf)\n",
        "#print(\"\\nnext run:\")\n",
        "#union1 = (getDecompUsingFreqTable(union,histograms[1]))\n",
        "\n",
        "#union2 = (getDecompUsingFreqTable(union1,histograms[2]))\n",
        "\n",
        "#print(\"after queries:\\n\")\n",
        "#union.show(10,False)\n",
        "#union1.show(10,False)\n",
        "#union2.show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "levWNTHzadFl"
      },
      "outputs": [],
      "source": [
        "#histograms[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "V-Ly_x_ladFl"
      },
      "outputs": [],
      "source": [
        "def addFeatureVector(df): #get feature vector for any dataframe for homogeneity function\n",
        "    string_cols = [c for c, t in df.dtypes if t =='string' and c != 'query'] #get all columns that have stringtype, except query column\n",
        "    \n",
        "    stringindex_cols = [(i + \"_indexed\") for i in string_cols]\n",
        "    indexer  = StringIndexer( inputCols=string_cols, outputCols=stringindex_cols, handleInvalid='error', stringOrderType='frequencyDesc')\n",
        "    indexer.setHandleInvalid(\"keep\") #change to \"skip\" to remove problematic rows\n",
        "    indexed = indexer.fit(df).transform(df) #dataframe with indexed columns attached\n",
        "    \n",
        "    allnonstringcols = [column.name for column in indexed.schema if column.dataType != StringType()]\n",
        "    vecAssembler = VectorAssembler(outputCol=\"features\")\n",
        "    \n",
        "    # normalizaing\n",
        "    #for col in allnonstringcols:\n",
        "    #    maxValue = indexed.select(max(col)).collect()[0][0]\n",
        "    #    minValue = indexed.select(min(col)).collect()[0][0]\n",
        "    #    indexed = indexed.withColumn(col + '_normalized', (indexed[col] - minValue)/(maxValue - minValue))\n",
        "    \n",
        "    #allnonstringcols = [col + '_normalized' for col in allnonstringcols]\n",
        "    vecAssembler.setInputCols(allnonstringcols) #all numerical columns are put into feature vector, including indexed cols\n",
        "    result=  ( vecAssembler.transform(indexed)) #return the dataframe with feature column attached\n",
        "    \n",
        "    for col in allnonstringcols:\n",
        "        result = result.drop(col)\n",
        "    for col in stringindex_cols:\n",
        "        result = result.drop(col)\n",
        "    return result\n",
        "\n",
        "#union2withvec = addFeatureVector(union2)\n",
        "#union2withvec.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eewCyg-PadFm"
      },
      "outputs": [],
      "source": [
        "#crossHomogeneityScore(union2withvec, 'query', 'features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WrV6J1nIadFm"
      },
      "outputs": [],
      "source": [
        "def shuffle(hist):\n",
        "    return sorted(hist, key = lambda x: x.count())\n",
        "\n",
        "\n",
        "def getDecompositionbyColumn(df, K):\n",
        "    '''\n",
        "    compute the final decomposition\n",
        "    \n",
        "    '''\n",
        "    histograms = shuffle(listOfFrequencyTables(df))\n",
        "    decomUnion = df\n",
        "    colLeft = len(histograms)\n",
        "    decomUnionWithVec = addFeatureVector(decomUnion)\n",
        "    overAllHomoScore = crossHomogeneityScore(decomUnionWithVec, 'query', 'features')\n",
        "    nBuckets = 1\n",
        "    \n",
        "    ifUpdated = True\n",
        "    while nBuckets <= K and colLeft > 0 and ifUpdated:\n",
        "        # at least one column left\n",
        "        # fewer groups than K\n",
        "\n",
        "        ifUpdated = False\n",
        "        \n",
        "        for freqdf in histograms:\n",
        "            \n",
        "            unionWithVec = getDecompUsingFreqTable(decomUnionWithVec, freqdf)\n",
        "            # unionWithVec.show()\n",
        "            crossScore =  crossHomogeneityScore(unionWithVec, 'query', 'features')\n",
        "            \n",
        "            nBuckets = len(unionWithVec.select('query').distinct().collect())\n",
        "            \n",
        "            if crossScore > overAllHomoScore and nBuckets <= K :\n",
        "                # \n",
        "                overAllHomoScore = crossScore\n",
        "                coldf = freqdf\n",
        "                ifUpdated = True\n",
        "                \n",
        "        if ifUpdated:\n",
        "            decomUnionWithVec = getDecompUsingFreqTable(decomUnionWithVec, freqdf)\n",
        "            histograms.remove(coldf)\n",
        "        colLeft = len(histograms)\n",
        "        \n",
        "    nBuckets = len(decomUnionWithVec.select('query').distinct().collect())\n",
        "    print('user requested K =', str(K), ', but we can only got ', str(nBuckets), 'clusters.') \n",
        "    return decomUnionWithVec.drop('features')\n",
        "            \n",
        "            \n",
        "        \n",
        "            \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "jYN5zxMzadFm"
      },
      "outputs": [],
      "source": [
        "#result = getDecompositionbyColumn(df, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dIru_fKadFm"
      },
      "outputs": [],
      "source": [
        "#result.show(result.count(), False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoZ3R5uoadFm",
        "outputId": "13ba5425-0377-48f5-bf51-e30ffb2d7681"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#result.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def newDecomp(df):\n",
        "  histograms = (listOfFrequencyTables(df))\n",
        "\n",
        "  numDistincts = []\n",
        "  for h in histograms: #put number of distinct items in each histogram into a list\n",
        "    numDistincts.append(h.count())\n",
        "  numDistincts.sort(reverse=True) #sort in descending order\n",
        "  midIdx = len(numDistincts)//2 #median value index\n",
        "  print(numDistincts)\n",
        "  for h in histograms:\n",
        "    if h.count() is numDistincts[midIdx]: \n",
        "      print(\"count=\",h.count())\n",
        "      midhist = h #histogram with median number of distinct values\n",
        "  midhist.show()\n",
        "  return midhist\n",
        "\n",
        "midhisto = newDecomp(df)\n",
        "queryresult = getDecompUsingFreqTable(df, midhisto)\n",
        "queryresult.show()\n",
        "decomUnionWithVec = addFeatureVector(queryresult)\n",
        "decomUnionWithVec.show()\n",
        "crossScore =  crossHomogeneityScore(decomUnionWithVec, 'query', 'features')\n",
        "print(crossScore)\n",
        "#newDecomp(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tbDAWODde9f",
        "outputId": "9e0b7e31-3566-4178-df63-1fe617e0142d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120, 53, 51, 47, 44, 25, 10, 10, 1, 1]\n",
            "count= 25\n",
            "+----+-----+\n",
            "|LatD|count|\n",
            "+----+-----+\n",
            "|41.0|   12|\n",
            "|38.0|   11|\n",
            "|42.0|   10|\n",
            "|39.0|   10|\n",
            "|37.0|    9|\n",
            "|43.0|    9|\n",
            "|44.0|    9|\n",
            "|32.0|    8|\n",
            "|40.0|    8|\n",
            "|33.0|    7|\n",
            "|46.0|    4|\n",
            "|47.0|    4|\n",
            "|34.0|    4|\n",
            "|36.0|    3|\n",
            "|35.0|    3|\n",
            "|31.0|    3|\n",
            "|49.0|    2|\n",
            "|29.0|    2|\n",
            "|45.0|    2|\n",
            "|27.0|    2|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "LatD=41.0\n",
            "LatD=38.0\n",
            "LatD=42.0\n",
            "LatD=39.0\n",
            "LatD=44.0\n",
            "LatD=37.0\n",
            "LatD=43.0\n",
            "LatD=40.0\n",
            "LatD=32.0\n",
            "LatD=33.0\n",
            "LatD=47.0\n",
            "LatD=34.0\n",
            "LatD=46.0\n",
            "LatD=35.0\n",
            "LatD=36.0\n",
            "LatD=31.0\n",
            "LatD=49.0\n",
            "LatD=29.0\n",
            "LatD=45.0\n",
            "LatD=27.0\n",
            "LatD=30.0\n",
            "LatD=50.0\n",
            "LatD=48.0\n",
            "LatD=28.0\n",
            "LatD=26.0\n",
            "+----+-------+-------+-----+-------+-------+-------+-----+---------------+--------+---------+\n",
            "|LatD| \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"|         \"City\"| \"State\"|    query|\n",
            "+----+-------+-------+-----+-------+-------+-------+-----+---------------+--------+---------+\n",
            "|41.0|    5.0|   59.0|  \"N\"|   80.0|   39.0|    0.0|  \"W\"|   \"Youngstown\"|      OH|LatD=41.0|\n",
            "|41.0|   15.0|    0.0|  \"N\"|   77.0|    0.0|    0.0|  \"W\"| \"Williamsport\"|      PA|LatD=41.0|\n",
            "|41.0|   25.0|   11.0|  \"N\"|  122.0|   23.0|   23.0|  \"W\"|         \"Weed\"|      CA|LatD=41.0|\n",
            "|41.0|   32.0|   59.0|  \"N\"|   73.0|    3.0|    0.0|  \"W\"|    \"Waterbury\"|      CT|LatD=41.0|\n",
            "|41.0|   50.0|   59.0|  \"N\"|   79.0|    8.0|   23.0|  \"W\"|       \"Warren\"|      PA|LatD=41.0|\n",
            "|41.0|   39.0|    0.0|  \"N\"|   83.0|   32.0|   24.0|  \"W\"|       \"Toledo\"|      OH|LatD=41.0|\n",
            "|41.0|   40.0|   48.0|  \"N\"|   86.0|   15.0|    0.0|  \"W\"|   \"South Bend\"|      IN|LatD=41.0|\n",
            "|41.0|   24.0|   35.0|  \"N\"|   75.0|   40.0|   11.0|  \"W\"|     \"Scranton\"|      PA|LatD=41.0|\n",
            "|41.0|   52.0|   11.0|  \"N\"|  103.0|   39.0|   36.0|  \"W\"|  \"Scottsbluff\"|      NB|LatD=41.0|\n",
            "|41.0|   27.0|    0.0|  \"N\"|   82.0|   42.0|   35.0|  \"W\"|     \"Sandusky\"|      OH|LatD=41.0|\n",
            "|41.0|   35.0|   24.0|  \"N\"|  109.0|   13.0|   48.0|  \"W\"| \"Rock Springs\"|      WY|LatD=41.0|\n",
            "|41.0|    9.0|   35.0|  \"N\"|   81.0|   14.0|   23.0|  \"W\"|      \"Ravenna\"|     OH |LatD=41.0|\n",
            "|38.0|   53.0|   23.0|  \"N\"|   77.0|    1.0|   47.0|  \"W\"|   \"Washington\"|      DC|LatD=38.0|\n",
            "|38.0|   40.0|   48.0|  \"N\"|   87.0|   31.0|   47.0|  \"W\"|    \"Vincennes\"|      IN|LatD=38.0|\n",
            "|38.0|    9.0|    0.0|  \"N\"|   79.0|    4.0|   11.0|  \"W\"|     \"Staunton\"|      VA|LatD=38.0|\n",
            "|38.0|   42.0|   35.0|  \"N\"|   93.0|   13.0|   48.0|  \"W\"|      \"Sedalia\"|      MO|LatD=38.0|\n",
            "|38.0|   26.0|   23.0|  \"N\"|  122.0|   43.0|   12.0|  \"W\"|   \"Santa Rosa\"|      CA|LatD=38.0|\n",
            "|38.0|   22.0|   11.0|  \"N\"|   75.0|   35.0|   59.0|  \"W\"|    \"Salisbury\"|      MD|LatD=38.0|\n",
            "|38.0|   50.0|   24.0|  \"N\"|   97.0|   36.0|   36.0|  \"W\"|       \"Salina\"|      KS|LatD=38.0|\n",
            "|38.0|   31.0|   47.0|  \"N\"|  106.0|    0.0|    0.0|  \"W\"|       \"Salida\"|      CO|LatD=38.0|\n",
            "+----+-------+-------+-----+-------+-------+-------+-----+---------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-----+---------------+--------+---------+--------------------+\n",
            "| \"NS\"| \"EW\"|         \"City\"| \"State\"|    query|            features|\n",
            "+-----+-----+---------------+--------+---------+--------------------+\n",
            "|  \"N\"|  \"W\"|   \"Youngstown\"|      OH|LatD=41.0|[41.0,5.0,59.0,80...|\n",
            "|  \"N\"|  \"W\"| \"Williamsport\"|      PA|LatD=41.0|(10,[0,1,3,8,9],[...|\n",
            "|  \"N\"|  \"W\"|         \"Weed\"|      CA|LatD=41.0|[41.0,25.0,11.0,1...|\n",
            "|  \"N\"|  \"W\"|    \"Waterbury\"|      CT|LatD=41.0|[41.0,32.0,59.0,7...|\n",
            "|  \"N\"|  \"W\"|       \"Warren\"|      PA|LatD=41.0|[41.0,50.0,59.0,7...|\n",
            "|  \"N\"|  \"W\"|       \"Toledo\"|      OH|LatD=41.0|[41.0,39.0,0.0,83...|\n",
            "|  \"N\"|  \"W\"|   \"South Bend\"|      IN|LatD=41.0|[41.0,40.0,48.0,8...|\n",
            "|  \"N\"|  \"W\"|     \"Scranton\"|      PA|LatD=41.0|[41.0,24.0,35.0,7...|\n",
            "|  \"N\"|  \"W\"|  \"Scottsbluff\"|      NB|LatD=41.0|[41.0,52.0,11.0,1...|\n",
            "|  \"N\"|  \"W\"|     \"Sandusky\"|      OH|LatD=41.0|[41.0,27.0,0.0,82...|\n",
            "|  \"N\"|  \"W\"| \"Rock Springs\"|      WY|LatD=41.0|[41.0,35.0,24.0,1...|\n",
            "|  \"N\"|  \"W\"|      \"Ravenna\"|     OH |LatD=41.0|[41.0,9.0,35.0,81...|\n",
            "|  \"N\"|  \"W\"|   \"Washington\"|      DC|LatD=38.0|[38.0,53.0,23.0,7...|\n",
            "|  \"N\"|  \"W\"|    \"Vincennes\"|      IN|LatD=38.0|[38.0,40.0,48.0,8...|\n",
            "|  \"N\"|  \"W\"|     \"Staunton\"|      VA|LatD=38.0|[38.0,9.0,0.0,79....|\n",
            "|  \"N\"|  \"W\"|      \"Sedalia\"|      MO|LatD=38.0|[38.0,42.0,35.0,9...|\n",
            "|  \"N\"|  \"W\"|   \"Santa Rosa\"|      CA|LatD=38.0|[38.0,26.0,23.0,1...|\n",
            "|  \"N\"|  \"W\"|    \"Salisbury\"|      MD|LatD=38.0|[38.0,22.0,11.0,7...|\n",
            "|  \"N\"|  \"W\"|       \"Salina\"|      KS|LatD=38.0|[38.0,50.0,24.0,9...|\n",
            "|  \"N\"|  \"W\"|       \"Salida\"|      CO|LatD=38.0|[38.0,31.0,47.0,1...|\n",
            "+-----+-----+---------------+--------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "0.7870427229712554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count_distinct\n",
        "decomUnionWithVec.select(count_distinct(\"query\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6MjhqpsxIjV",
        "outputId": "f5ca5233-da07-4ea5-dac7-cdaf3e1a1597"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|count(DISTINCT query)|\n",
            "+---------------------+\n",
            "|                   25|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "decomposition_medianonly.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}