{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SPARK_HOME\"] = \"/home/com3dian/Documents/github/Period4/DIS/spark-3.2.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## You can add more config while building \n",
    "spark = SparkSession.builder.master(\"local[8]\").\\\n",
    "                    config(\"spark.app.name\",\"session_one\").\\\n",
    "                    getOrCreate() #number of threads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2NumpyArray(df, colName):\n",
    "    '''\n",
    "    convert spark dataframe to numpy array\n",
    "    '''\n",
    "    return np.array(df.select(colName).collect())\n",
    "\n",
    "\n",
    "def numpyArray2Matrix(array):\n",
    "    '''\n",
    "    convert numpy array to spark Rowmatrix\n",
    "    ----------------------------\n",
    "    return: Rowmatrix\n",
    "    '''\n",
    "    if len(array.shape) == 3:\n",
    "        array.reshape((array.shape[0], array.shape[-1]))\n",
    "    \n",
    "    \n",
    "    denseVectorList = []\n",
    "    for i in range(len(array)):\n",
    "        denseVectorList.append(Vectors.dense(array[i]))\n",
    "        \n",
    "    \n",
    "    RDD = spark.sparkContext.parallelize(denseVectorList)\n",
    "    normVectors = RDD.map(lambda x: x/(np.linalg.norm(x, 2)))\n",
    "    \n",
    "    RDD = spark.sparkContext.parallelize(normVectors.collect())\n",
    "    matrix = RowMatrix(RDD)\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "def SVDsimilarity(matrix, numDimension = 1, normalization = False):\n",
    "    '''\n",
    "    generalized cosine similarity using SVD(singular value decomposition)\n",
    "    by doing SVD, the input matrix Y will be decomposited into 3 matrix: U, S, V, with Y = USV^T\n",
    "    where S can be considered as a lower rank approximation of Y\n",
    "    the SVD optimal in the sense that minimizing the Frobinius norm of reconstruction error || \\hat{Y} - Y ||^{2}_{F}\n",
    "    therefore, by comparing the 'order K coefficient of determination' \\frac{||\\hat{Y} ||^2_F}{||Y ||^2_F}, we shall a similarity.\n",
    "    ----------------------\n",
    "    in the case of only 2 vectors, the SVD similarity is equal to the cosine similarity\n",
    "    ----------------------\n",
    "    the original SVD similarity is ranged from 1/n to 1, where n is the number of vectors\n",
    "    to get it can range over the entire [0,1] interval, one can normalize it by \\frac{}{} if only using the first sigular value\n",
    "    \n",
    "    ------------------------------------------\n",
    "    matrix: pyspark RowMatrix, represents a row-oriented distributed Matrix with no meaningful row indices\n",
    "            each column/row is an input vector\n",
    "            all element in matrix should be positive\n",
    "    numDimension: integer, if not 1 then use the first(largest) few singular value\n",
    "    normalization: if true then do normalization\n",
    "    \n",
    "    '''\n",
    "    N = matrix.numRows()\n",
    "    # SVD\n",
    "    svd = matrix.computeSVD(numDimension, computeU=False)\n",
    "    sVector = svd.s.toArray()\n",
    "    YApproximate = np.sum(sVector*sVector)**0.5\n",
    "    \n",
    "    GramianMatrix = matrix.computeGramianMatrix().toArray()\n",
    "    Y = np.trace(GramianMatrix)**0.5\n",
    "    \n",
    "    # normalization\n",
    "    if not normalization:\n",
    "        similarityScore = YApproximate/Y\n",
    "    else:\n",
    "        similarityScore = ((YApproximate/Y * N) - 1)/(N -1)\n",
    "        \n",
    "    # return 2 * similarityScore**2 - 1\n",
    "    # double angle formula\n",
    "    return 2 * similarityScore**2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossHomogeneityScore(df, queryColName, featureColName):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if queryColName not in df.schema.names:\n",
    "        \n",
    "        npArray = dataframe2NumpyArray(df, featureColName)\n",
    "        matrix = numpyArray2Matrix(npArray)\n",
    "        similarity = SVDsimilarity(matrix)\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    \n",
    "    totalRows = df.count()\n",
    "    queries = list(set(df.select(queryColName).collect()))\n",
    "    \n",
    "    homogeneityScore = 0\n",
    "    \n",
    "    for query in queries:\n",
    "        # get each cluster\n",
    "        dfQuery = df.filter(df[queryColName] == query.query)\n",
    "        # get number of rows\n",
    "        numRows = dfQuery.count()\n",
    "        \n",
    "        npArray = dataframe2NumpyArray(dfQuery, featureColName)\n",
    "        matrix = numpyArray2Matrix(npArray)\n",
    "        \n",
    "        similarity = SVDsimilarity(matrix)\n",
    "        homogeneityScore += similarity * numRows/totalRows\n",
    "    \n",
    "    return homogeneityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+\n",
      "|Name|     city|age|\n",
      "+----+---------+---+\n",
      "|John|  utrecht| 20|\n",
      "|Mary|amsterdam| 21|\n",
      "|Nick|rotterdam| 22|\n",
      "|Nick|  utrecht| 21|\n",
      "|John|rotterdam| 21|\n",
      "|Mary|rotterdam| 22|\n",
      "|Nick|  utrecht| 20|\n",
      "|John|rotterdam| 21|\n",
      "|Nick|amsterdam| 20|\n",
      "|Mary|  utrecht| 22|\n",
      "+----+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/com3dian/Documents/github/DIS_project_2022/data/testpeople.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "|  utrecht|    4|\n",
      "|rotterdam|    4|\n",
      "|amsterdam|    2|\n",
      "+---------+-----+\n",
      "\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 21|    4|\n",
      "| 22|    3|\n",
      "| 20|    3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def listOfFrequencyTables(df): #take main dataframe, generate frequency dataframes\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    histograms = []\n",
    "    for col in df.dtypes:\n",
    "        h=df.groupBy(col[0]).count()\n",
    "        h = h.sort(desc(\"count\"))\n",
    "        histograms.append(h)\n",
    "        h.show() #comment this line to suppress output\n",
    "    return histograms\n",
    "\n",
    "histograms = listOfFrequencyTables(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name = Nick\n",
      "+----+---------+---+\n",
      "|Name|     city|age|\n",
      "+----+---------+---+\n",
      "|Nick|rotterdam| 22|\n",
      "|Nick|  utrecht| 21|\n",
      "|Nick|  utrecht| 20|\n",
      "|Nick|amsterdam| 20|\n",
      "+----+---------+---+\n",
      "\n",
      "city = rotterdam\n",
      "+----+---------+---+\n",
      "|Name|     city|age|\n",
      "+----+---------+---+\n",
      "|Nick|rotterdam| 22|\n",
      "|John|rotterdam| 21|\n",
      "|Mary|rotterdam| 22|\n",
      "|John|rotterdam| 21|\n",
      "+----+---------+---+\n",
      "\n",
      "age = 21\n",
      "+----+---------+---+\n",
      "|Name|     city|age|\n",
      "+----+---------+---+\n",
      "|Mary|amsterdam| 21|\n",
      "|Nick|  utrecht| 21|\n",
      "|John|rotterdam| 21|\n",
      "|John|rotterdam| 21|\n",
      "+----+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getDecompFromTopFrequencies(df, histograms):\n",
    "    clusterlst=[]\n",
    "    for i in range(len(histograms)): #query database with top values of all columns\n",
    "        d= str(histograms[i].first()) #value of the first row\n",
    "        #print(d)\n",
    "        d = d.split(\",\")[0].split('=')[1] #the splits are for formatting the string\n",
    "        #print(d)\n",
    "        #print(\"d before:\",d)\n",
    "        if \"'\"  in d:\n",
    "            d = d.split(\"'\")[1]\n",
    "        \n",
    "        #print(\"d after:\",d)\n",
    "        #print(type(d))\n",
    "        cname = str(histograms[i][0]).split(\"'\")[1]\n",
    "        print(cname,\"=\",d)\n",
    "        \n",
    "        data = (df.filter(col(cname) == d))\n",
    "        \n",
    "        data.show(15)\n",
    "        clusterlst.append(data)\n",
    "    return clusterlst\n",
    "\n",
    "clusterlst = getDecompFromTopFrequencies(df, histograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is frequency table:\n",
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n",
      "querystr: Name=Nick\n",
      "creating query column:\n",
      "Name=Nick\n",
      "querystr: Name=Mary\n",
      "creating query column:\n",
      "Name=Mary\n",
      "querystr: Name=John\n",
      "creating query column:\n",
      "Name=John\n",
      "equality result:\n",
      "\n",
      "next run:\n",
      "this is frequency table:\n",
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "|  utrecht|    4|\n",
      "|rotterdam|    4|\n",
      "|amsterdam|    2|\n",
      "+---------+-----+\n",
      "\n",
      "querystr: city=rotterdam\n",
      "first newres:\n",
      "after join\n",
      "city=rotterdam\n",
      "querystr: city=utrecht\n",
      "first newres:\n",
      "after join\n",
      "city=utrecht\n",
      "querystr: city=amsterdam\n",
      "first newres:\n",
      "after join\n",
      "city=amsterdam\n",
      "this is frequency table:\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 21|    4|\n",
      "| 22|    3|\n",
      "| 20|    3|\n",
      "+---+-----+\n",
      "\n",
      "querystr: age=21\n",
      "first newres:\n",
      "after join\n",
      "age=21\n",
      "querystr: age=22\n",
      "first newres:\n",
      "after join\n",
      "age=22\n",
      "querystr: age=20\n",
      "first newres:\n",
      "after join\n",
      "age=20\n",
      "after queries:\n",
      "\n",
      "+----+---------+---+---------+\n",
      "|Name|city     |age|query    |\n",
      "+----+---------+---+---------+\n",
      "|Nick|rotterdam|22 |Name=Nick|\n",
      "|Nick|utrecht  |21 |Name=Nick|\n",
      "|Nick|utrecht  |20 |Name=Nick|\n",
      "|Nick|amsterdam|20 |Name=Nick|\n",
      "|Mary|amsterdam|21 |Name=Mary|\n",
      "|Mary|rotterdam|22 |Name=Mary|\n",
      "|Mary|utrecht  |22 |Name=Mary|\n",
      "|John|utrecht  |20 |Name=John|\n",
      "|John|rotterdam|21 |Name=John|\n",
      "|John|rotterdam|21 |Name=John|\n",
      "+----+---------+---+---------+\n",
      "\n",
      "+----+---------+---+------------------------+\n",
      "|Name|city     |age|query                   |\n",
      "+----+---------+---+------------------------+\n",
      "|Nick|rotterdam|22 |Name=Nick,city=rotterdam|\n",
      "|Mary|rotterdam|22 |Name=Mary,city=rotterdam|\n",
      "|John|rotterdam|21 |Name=John,city=rotterdam|\n",
      "|John|rotterdam|21 |Name=John,city=rotterdam|\n",
      "|Nick|utrecht  |21 |Name=Nick,city=utrecht  |\n",
      "|Nick|utrecht  |20 |Name=Nick,city=utrecht  |\n",
      "|Mary|utrecht  |22 |Name=Mary,city=utrecht  |\n",
      "|John|utrecht  |20 |Name=John,city=utrecht  |\n",
      "|Nick|amsterdam|20 |Name=Nick,city=amsterdam|\n",
      "|Mary|amsterdam|21 |Name=Mary,city=amsterdam|\n",
      "+----+---------+---+------------------------+\n",
      "\n",
      "+----+---------+---+-------------------------------+\n",
      "|Name|city     |age|query                          |\n",
      "+----+---------+---+-------------------------------+\n",
      "|John|rotterdam|21 |Name=John,city=rotterdam,age=21|\n",
      "|John|rotterdam|21 |Name=John,city=rotterdam,age=21|\n",
      "|Nick|utrecht  |21 |Name=Nick,city=utrecht,age=21  |\n",
      "|Mary|amsterdam|21 |Name=Mary,city=amsterdam,age=21|\n",
      "|Nick|rotterdam|22 |Name=Nick,city=rotterdam,age=22|\n",
      "|Mary|rotterdam|22 |Name=Mary,city=rotterdam,age=22|\n",
      "|Mary|utrecht  |22 |Name=Mary,city=utrecht,age=22  |\n",
      "|Nick|utrecht  |20 |Name=Nick,city=utrecht,age=20  |\n",
      "|John|utrecht  |20 |Name=John,city=utrecht,age=20  |\n",
      "|Nick|amsterdam|20 |Name=Nick,city=amsterdam,age=20|\n",
      "+----+---------+---+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def are_dfs_equal(df1, df2): #this works, i tested it\n",
    "    res = df1.subtract(df2) #set subtraction on the two dataframes. \n",
    "    if res.count() == 0: #subtraction yielded empty set\n",
    "        print(\"dataframes are equal\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"error! these rows are not in the union of your queries:\")\n",
    "        res.show() #show which tuples are not included in your query union\n",
    "        return False\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def getDecompUsingFreqTable(df,freqdf): #takes original database and one frequency table as input, returns union of all queried dataframes as output\n",
    "    print(\"this is frequency table:\")\n",
    "    freqdf.show()\n",
    "    cname = freqdf.columns[0]\n",
    "    valuelist = (freqdf.select(freqdf.columns[0]).rdd.flatMap(lambda x: x).collect()) #list of all values of frequency column\n",
    "    \n",
    "    unionlst = []\n",
    "    \n",
    "    for v in valuelist: #each unique value in the freq. table is used as a query\n",
    "        result = df.filter(col(cname) == v)\n",
    "        querystr = cname + \"=\" + str(v)\n",
    "        print(\"querystr:\",querystr)\n",
    "        \n",
    "        containsquery = False\n",
    "        for c in df.columns: #check if query column exists in the input dataframe\n",
    "            if \"query\" in c:\n",
    "                containsquery= True\n",
    "        \n",
    "        if(containsquery): #check if query column already exists in the input\n",
    "            result=result.withColumn(\"query1\",lit(querystr))\n",
    "            print(\"first newres:\")\n",
    "            \n",
    "            result= result.withColumn(\"joined\",concat(concat(col(\"query\"), lit(\",\"), col(\"query1\")))) #putting query with existing queries\n",
    "            print(\"after join\")\n",
    "            \n",
    "            columns_to_drop = ['query', 'query1']\n",
    "            \n",
    "            result=result.drop('query')\n",
    "            result=result.drop('query1')\n",
    "            result=result.withColumnRenamed(\"joined\",\"query\")\n",
    "            \n",
    "            result=result\n",
    "        else:\n",
    "            print(\"creating query column:\")\n",
    "            result=result.withColumn(\"query\",lit(querystr))\n",
    "        print(querystr)\n",
    "        \n",
    "        unionlst.append(result)\n",
    "    unn = reduce(DataFrame.unionAll, unionlst) #put all queried dataframes back together as one\n",
    "    \n",
    "    \n",
    "    return unn\n",
    "\n",
    "\n",
    "union = (getDecompUsingFreqTable(df,histograms[0])) #function call with 'Name' frequency table\n",
    "print(\"equality result:\")\n",
    "#subdf = are_dfs_equal(df,union.drop('query')) #checks if union of queries covers whole database\n",
    "#print(subdf)\n",
    "print(\"\\nnext run:\")\n",
    "union1 = (getDecompUsingFreqTable(union,histograms[1]))\n",
    "\n",
    "union2 = (getDecompUsingFreqTable(union1,histograms[2]))\n",
    "\n",
    "print(\"after queries:\\n\")\n",
    "union.show(10,False)\n",
    "union1.show(10,False)\n",
    "union2.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "histograms[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+--------------------+-------------+\n",
      "|Name|     city|age|               query|     features|\n",
      "+----+---------+---+--------------------+-------------+\n",
      "|John|rotterdam| 21|Name=John,city=ro...|[0.5,0.5,0.0]|\n",
      "|John|rotterdam| 21|Name=John,city=ro...|[0.5,0.5,0.0]|\n",
      "|Nick|  utrecht| 21|Name=Nick,city=ut...|[0.5,0.0,0.5]|\n",
      "|Mary|amsterdam| 21|Name=Mary,city=am...|[0.5,1.0,1.0]|\n",
      "|Nick|rotterdam| 22|Name=Nick,city=ro...|[1.0,0.0,0.0]|\n",
      "|Mary|rotterdam| 22|Name=Mary,city=ro...|[1.0,1.0,0.0]|\n",
      "|Mary|  utrecht| 22|Name=Mary,city=ut...|[1.0,1.0,0.5]|\n",
      "|Nick|  utrecht| 20|Name=Nick,city=ut...|[0.0,0.0,0.5]|\n",
      "|John|  utrecht| 20|Name=John,city=ut...|[0.0,0.5,0.5]|\n",
      "|Nick|amsterdam| 20|Name=Nick,city=am...|[0.0,0.0,1.0]|\n",
      "+----+---------+---+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def addFeatureVector(df): #get feature vector for any dataframe for homogeneity function\n",
    "    string_cols = [c for c, t in df.dtypes if t =='string' and c != 'query'] #get all columns that have stringtype, except query column\n",
    "    \n",
    "    stringindex_cols = [(i + \"_indexed\") for i in string_cols]\n",
    "    indexer  = StringIndexer( inputCols=string_cols, outputCols=stringindex_cols, handleInvalid='error', stringOrderType='frequencyDesc')\n",
    "    indexer.setHandleInvalid(\"keep\") #change to \"skip\" to remove problematic rows\n",
    "    indexed = indexer.fit(df).transform(df) #dataframe with indexed columns attached\n",
    "    \n",
    "    allnonstringcols = [column.name for column in indexed.schema if column.dataType != StringType()]\n",
    "    vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "    \n",
    "    # normalizaing\n",
    "    for col in allnonstringcols:\n",
    "        maxValue = indexed.select(max(col)).collect()[0][0]\n",
    "        minValue = indexed.select(min(col)).collect()[0][0]\n",
    "        indexed = indexed.withColumn(col + '_normalized', (indexed[col] - minValue)/(maxValue - minValue))\n",
    "    \n",
    "    allnonstringcols = [col + '_normalized' for col in allnonstringcols]\n",
    "    vecAssembler.setInputCols(allnonstringcols) #all numerical columns are put into feature vector, including indexed cols\n",
    "    result=  ( vecAssembler.transform(indexed)) #return the dataframe with feature column attached\n",
    "    \n",
    "    for col in allnonstringcols:\n",
    "        result = result.drop(col)\n",
    "    for col in stringindex_cols:\n",
    "        result = result.drop(col)\n",
    "    return result\n",
    "\n",
    "union2withvec = addFeatureVector(union2)\n",
    "union2withvec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossHomogeneityScore(union2withvec, 'query', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(hist):\n",
    "    return sorted(hist, key = lambda x: x.count())\n",
    "\n",
    "\n",
    "def getDecompositionbyColumn(df, K):\n",
    "    '''\n",
    "    compute the final decomposition\n",
    "    \n",
    "    '''\n",
    "    histograms = shuffle(listOfFrequencyTables(df))\n",
    "    decomUnion = df\n",
    "    colLeft = len(histograms)\n",
    "    decomUnionWithVec = addFeatureVector(decomUnion)\n",
    "    overAllHomoScore = crossHomogeneityScore(decomUnionWithVec, 'query', 'features')\n",
    "    nBuckets = 1\n",
    "    \n",
    "    ifUpdated = True\n",
    "    while nBuckets <= K and colLeft > 0 and ifUpdated:\n",
    "        # at least one column left\n",
    "        # fewer groups than K\n",
    "\n",
    "        ifUpdated = False\n",
    "        \n",
    "        for freqdf in histograms:\n",
    "            \n",
    "            unionWithVec = getDecompUsingFreqTable(decomUnionWithVec, freqdf)\n",
    "            # unionWithVec.show()\n",
    "            crossScore =  crossHomogeneityScore(unionWithVec, 'query', 'features')\n",
    "            \n",
    "            nBuckets = len(unionWithVec.select('query').distinct().collect())\n",
    "            \n",
    "            if crossScore > overAllHomoScore and nBuckets <= K :\n",
    "                # \n",
    "                overAllHomoScore = crossScore\n",
    "                coldf = freqdf\n",
    "                ifUpdated = True\n",
    "                \n",
    "        if ifUpdated:\n",
    "            decomUnionWithVec = getDecompUsingFreqTable(decomUnionWithVec, freqdf)\n",
    "            histograms.remove(coldf)\n",
    "        colLeft = len(histograms)\n",
    "        \n",
    "    nBuckets = len(decomUnionWithVec.select('query').distinct().collect())\n",
    "    print('user requested K =', str(K), ', but we can only got ', str(nBuckets), 'clusters.') if nBuckets != K\n",
    "    return decomUnionWithVec.drop('features')\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "|  utrecht|    4|\n",
      "|rotterdam|    4|\n",
      "|amsterdam|    2|\n",
      "+---------+-----+\n",
      "\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 21|    4|\n",
      "| 22|    3|\n",
      "| 20|    3|\n",
      "+---+-----+\n",
      "\n",
      "this is frequency table:\n",
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n",
      "querystr: Name=Nick\n",
      "creating query column:\n",
      "Name=Nick\n",
      "querystr: Name=Mary\n",
      "creating query column:\n",
      "Name=Mary\n",
      "querystr: Name=John\n",
      "creating query column:\n",
      "Name=John\n",
      "this is frequency table:\n",
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "|  utrecht|    4|\n",
      "|rotterdam|    4|\n",
      "|amsterdam|    2|\n",
      "+---------+-----+\n",
      "\n",
      "querystr: city=rotterdam\n",
      "creating query column:\n",
      "city=rotterdam\n",
      "querystr: city=utrecht\n",
      "creating query column:\n",
      "city=utrecht\n",
      "querystr: city=amsterdam\n",
      "creating query column:\n",
      "city=amsterdam\n",
      "this is frequency table:\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 21|    4|\n",
      "| 22|    3|\n",
      "| 20|    3|\n",
      "+---+-----+\n",
      "\n",
      "querystr: age=21\n",
      "creating query column:\n",
      "age=21\n",
      "querystr: age=22\n",
      "creating query column:\n",
      "age=22\n",
      "querystr: age=20\n",
      "creating query column:\n",
      "age=20\n",
      "this is frequency table:\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 21|    4|\n",
      "| 22|    3|\n",
      "| 20|    3|\n",
      "+---+-----+\n",
      "\n",
      "querystr: age=21\n",
      "creating query column:\n",
      "age=21\n",
      "querystr: age=22\n",
      "creating query column:\n",
      "age=22\n",
      "querystr: age=20\n",
      "creating query column:\n",
      "age=20\n",
      "this is frequency table:\n",
      "+----+-----+\n",
      "|Name|count|\n",
      "+----+-----+\n",
      "|Nick|    4|\n",
      "|Mary|    3|\n",
      "|John|    3|\n",
      "+----+-----+\n",
      "\n",
      "querystr: Name=Nick\n",
      "first newres:\n",
      "after join\n",
      "Name=Nick\n",
      "querystr: Name=Mary\n",
      "first newres:\n",
      "after join\n",
      "Name=Mary\n",
      "querystr: Name=John\n",
      "first newres:\n",
      "after join\n",
      "Name=John\n",
      "this is frequency table:\n",
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "|  utrecht|    4|\n",
      "|rotterdam|    4|\n",
      "|amsterdam|    2|\n",
      "+---------+-----+\n",
      "\n",
      "querystr: city=rotterdam\n",
      "first newres:\n",
      "after join\n",
      "city=rotterdam\n",
      "querystr: city=utrecht\n",
      "first newres:\n",
      "after join\n",
      "city=utrecht\n",
      "querystr: city=amsterdam\n",
      "first newres:\n",
      "after join\n",
      "city=amsterdam\n",
      "user requested K = 3 , but we got  3 clusters.\n"
     ]
    }
   ],
   "source": [
    "result = getDecompositionbyColumn(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+------+\n",
      "|Name|city     |age|query |\n",
      "+----+---------+---+------+\n",
      "|Mary|amsterdam|21 |age=21|\n",
      "|Nick|utrecht  |21 |age=21|\n",
      "|John|rotterdam|21 |age=21|\n",
      "|John|rotterdam|21 |age=21|\n",
      "|Nick|rotterdam|22 |age=22|\n",
      "|Mary|rotterdam|22 |age=22|\n",
      "|Mary|utrecht  |22 |age=22|\n",
      "|John|utrecht  |20 |age=20|\n",
      "|Nick|utrecht  |20 |age=20|\n",
      "|Nick|amsterdam|20 |age=20|\n",
      "+----+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(result.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
