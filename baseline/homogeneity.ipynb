{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SPARK_HOME\"] = \"/home/com3dian/Documents/github/Period4/DIS/spark-3.2.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## You can add more config while building \n",
    "spark = SparkSession.builder.master(\"local[8]\").\\\n",
    "                    config(\"spark.app.name\",\"session_one\").\\\n",
    "                    getOrCreate() #number of threads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import numpy as np\n",
    "\n",
    "def dataframe2NumpyArray(df, colName):\n",
    "    '''\n",
    "    convert spark dataframe to numpy array\n",
    "    '''\n",
    "    return np.array(df.select(colName).collect())\n",
    "    \n",
    "def numpyArray2Matrix(array):\n",
    "    '''\n",
    "    convert numpy array to spark Rowmatrix\n",
    "    ----------------------------\n",
    "    return: Rowmatrix\n",
    "    '''\n",
    "    if len(array.shape) == 3:\n",
    "        array.reshape((array.shape[0], array.shape[-1]))\n",
    "    \n",
    "    \n",
    "    denseVectorList = []\n",
    "    for i in range(len(array)):\n",
    "        denseVectorList.append(Vectors.dense(array[i]))\n",
    "    \n",
    "    RDD = spark.sparkContext.parallelize(denseVectorList)\n",
    "    matrix = RowMatrix(RDD)\n",
    "        \n",
    "    return matrix\n",
    "    \n",
    "def SVDsimilarity(matrix, numDimension = 1, normalization = False):\n",
    "    '''\n",
    "    generalized cosine similarity using SVD(singular value decomposition)\n",
    "    by doing SVD, the input matrix Y will be decomposited into 3 matrix: U, S, V, with Y = USV^T\n",
    "    where S can be considered as a lower rank approximation of Y\n",
    "    the SVD optimal in the sense that minimizing the Frobinius norm of reconstruction error || \\hat{Y} - Y ||^{2}_{F}\n",
    "    therefore, by comparing the 'order K coefficient of determination' \\frac{||\\hat{Y} ||^2_F}{||Y ||^2_F}, we shall a similarity.\n",
    "    ----------------------\n",
    "    in the case of only 2 vectors, the SVD similarity is equal to the cosine similarity\n",
    "    ----------------------\n",
    "    the original SVD similarity is ranged from 1/n to 1, where n is the number of vectors\n",
    "    to get it can range over the entire [0,1] interval, one can normalize it by \\frac{}{} if only using the first sigular value\n",
    "    \n",
    "    ------------------------------------------\n",
    "    matrix: pyspark RowMatrix, represents a row-oriented distributed Matrix with no meaningful row indices\n",
    "            each column is an input vector\n",
    "            all element in matrix should be positive\n",
    "    numDimension: integer, if not 1 then use the first(largest) few singular value\n",
    "    normalization: if true then do normalization\n",
    "    \n",
    "    '''\n",
    "    N = matrix.numRows()\n",
    "    # SVD\n",
    "    svd = matrix.computeSVD(numDimension, computeU=False)\n",
    "    sVector = svd.s.toArray()\n",
    "    print(sVector)\n",
    "    YApproximate = sum(sVector*sVector)**0.5\n",
    "    \n",
    "    GramianMatrix = matrix.computeGramianMatrix().toArray()\n",
    "    Y = np.trace(GramianMatrix)**0.5\n",
    "    \n",
    "    # normalization\n",
    "    print(YApproximate)\n",
    "    print(Y)\n",
    "    if not normalization:\n",
    "        similarityScore = YApproximate/Y\n",
    "    else:\n",
    "        similarityScore = ((YApproximate/Y * N) - 1)/(N -1)\n",
    "        \n",
    "    # return 2 * similarityScore**2 - 1\n",
    "    return 2 * similarityScore**2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.48808848]\n",
      "10.488088481701515\n",
      "10.488088481701515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = spark.sparkContext.parallelize([\n",
    "    Vectors.dense(4.0, 2.0),\n",
    "    Vectors.dense(2.0, 1.0),\n",
    "    Vectors.dense(6.0, 3.0),\n",
    "    Vectors.dense(4.0, 2.0),\n",
    "    Vectors.dense(4.0, 2.0)\n",
    "])\n",
    "mat = RowMatrix(RDD)\n",
    "\n",
    "SVDsimilarity(mat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.22474487]\n",
      "1.224744871391589\n",
      "1.4142135623730951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49999999999999933"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = spark.sparkContext.parallelize([\n",
    "    Vectors.dense(0.5, 0.5*3**.5),\n",
    "    Vectors.dense(1.0, 0.0)\n",
    "])\n",
    "mat = RowMatrix(RDD)\n",
    "\n",
    "SVDsimilarity(mat, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
